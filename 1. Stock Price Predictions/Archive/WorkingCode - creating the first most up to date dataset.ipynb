{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f51aae0",
   "metadata": {},
   "source": [
    "# Combining Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3cb5e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "67bdf18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in every csv file we have manually saved \n",
    "path = r'C:\\\\Users\\\\mball3\\\\OneDrive - KPMG\\\\Documents\\\\Investment_Analysis\\\\data\\\\18_12_2023'\n",
    "filenames = glob.glob(path+\"/*.csv\")\n",
    "\n",
    "# This specifies to the url below, which devices that url can be accessed from\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "    ############################################\n",
    "    # Read in the historical data for each stock \n",
    "    historical_data = pd.read_csv(filename)\n",
    "    stock = filename.split(\"\\\\\")[-1].split('.')[0]\n",
    "    \n",
    "    # Convert date to date format - english \n",
    "    historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n",
    "    \n",
    "    ############################################\n",
    "    # Read in the new data\n",
    "    ############################################\n",
    "    \n",
    "    # Set the URL to the current stock's prices\n",
    "    url = (f\"https://finance.yahoo.com/quote/{stock}/history?p={stock}\")\n",
    "    r = requests.get(url, verify=False, headers=headers)\n",
    "\n",
    "    # This div class contains all the information I need on hisorical stock price data\n",
    "    web_content = BeautifulSoup(r.text, 'lxml')\n",
    "    web_content = web_content.find('div', class_='Pb(10px) Ovx(a) W(100%)')\n",
    "\n",
    "    # within the div class there is a tbody \n",
    "    tbody = web_content.find(\"tbody\")\n",
    "\n",
    "    # each tr is a row within the tbody, which contains information for every date\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "\n",
    "    # Define the headers and create an empty dictionary to fill\n",
    "    data_headers = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\n",
    "    data_dict = {'Date': [], 'Open': [], 'High': [], 'Low': [], 'Close': [], 'Adj Close': [], 'Volume': []}\n",
    "\n",
    "    # This gets every row from tbody - ignoring the top row as they are headers\n",
    "    for row in rows:\n",
    "        data_row = []\n",
    "        # Each td is a different value within that row\n",
    "        for cell in row.find_all(\"td\"):\n",
    "            # Strip removes unwanted info\n",
    "            row_data = cell.text.strip()\n",
    "            # Store this for this row \n",
    "            data_row.append(row_data)\n",
    "\n",
    "        # This then zips together the data with the header\n",
    "        # Which then populates the dictionary, ensuring that the data goes into the correct place\n",
    "        for header, value in zip(data_headers, data_row):\n",
    "            if len(data_row) == 7:\n",
    "                data_dict[header].append(value)\n",
    "\n",
    "            continue \n",
    "\n",
    "\n",
    "    # Convert to date\n",
    "    scraped_data = pd.DataFrame(data_dict)\n",
    "    scraped_data['Date'] = pd.to_datetime(scraped_data['Date'])\n",
    "    \n",
    "    # Add new data to the end of historical data\n",
    "    new_data = scraped_data[scraped_data['Date'] > max(historical_data['Date'])]\n",
    "    full_data = pd.concat([historical_data,new_data], axis=0)\n",
    "    \n",
    "    # Change data types\n",
    "    full_data['Volume'] = full_data['Volume'].replace(',', '', regex=True)\n",
    "    full_data[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']] = full_data[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']].apply(pd.to_numeric, errors='coerce')\n",
    " \n",
    "    ############################################\n",
    "    # Now save this file in a folder with todays date\n",
    "    ############################################\n",
    "    latest_date = datetime.now() - timedelta(days=1)\n",
    "    latest_date = latest_date.strftime('%d_%m_%Y')\n",
    "    folder_path = os.path.join('C:\\\\Users\\\\mball3\\\\OneDrive - KPMG\\\\Documents\\\\Investment_Analysis\\\\data', latest_date)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    final_file_path = os.path.join(folder_path, f'{stock}.csv')\n",
    "    full_data.to_csv(final_file_path, index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7d4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
